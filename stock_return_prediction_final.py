# -*- coding: utf-8 -*-
"""“stock_return_prediction .ipynb”的副本

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EuSeY0ufZjiA2nahlGcHp0lCgMEgtA6n
"""

!pip install bayesian-optimization

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import math
import os
import statsmodels.api as sm
import lightgbm as lgb
import xgboost as xgb
import matplotlib.pyplot as plt
from sklearn.linear_model import Lasso
from sklearn.linear_model import RidgeCV, LassoCV
from sklearn.model_selection import train_test_split, cross_val_score, KFold, TimeSeriesSplit, GroupKFold, StratifiedKFold
from sklearn.linear_model import LinearRegression
import joblib
from sklearn.metrics import mean_squared_error, r2_score
from bayes_opt import BayesianOptimization


import warnings
warnings.filterwarnings('ignore')

# Set the seed for both Python's hash function and NumPy's random number generator to a specified value.
def seed_everything(seed: int) -> None:
    # Set the seed for Python's hash function
    os.environ['PYTHONHASHSEED'] = str(seed)  
    # Set the seed for NumPy's random number generator
    np.random.seed(seed)  

# Set the seed to 4 and call the seed_everything() function to set the seed for Python and NumPy
SEED=4
seed_everything(SEED)

df4=pd.read_csv('/content/drive/MyDrive/DF4.csv')

DF1=pd.read_csv('/content/drive/MyDrive/DF1.csv')
DF1

df4.shape



all_stock_name=df4['windCode'].unique()

all_stock_name.shape

stock_for_predict_model = np.random.choice(all_stock_name, size=219, replace=False)
DF = df4[df4['windCode'].isin(stock_for_predict_model)]
backtest_data = df4[df4['windCode'].isin(stock_for_predict_model)]

backtest_data.to_csv('/content/drive/MyDrive/backtest_data.csv')

"""### 引入下一天的return作为预测目标，删除了最后一天的数据避免NAN"""

# Create a new column called Next-Return
DF1=DF.copy()
DF1=DF1.sort_values(['windCode', 'tradingDate'])
DF1["Next-Return"] = DF1.groupby('windCode')['return'].shift(-1)

# Delete the last day of data for each stock
DF1 = DF1.groupby("windCode").apply(lambda x: x.iloc[:-1]).reset_index(drop=True).sort_values(['windCode', 'tradingDate'])

DF1.isnull().sum()

pd.set_option('display.max_rows', 200)

DF1 = DF1[DF1['alpha114'] != np.inf]

inf_codes = DF.loc[np.isinf(DF['alpha114']), ['windCode', 'tradingDate']].drop_duplicates()

inf_codes

DF[DF['windCode']=='002916.XSHE']['alpha114']

DF1

DF1.to_csv('/content/drive/MyDrive/DF1.csv')

def feval_rmse(y_pred, lgb_train):
    y_true = lgb_train.get_label()
    return 'rmse', mean_squared_error(y_true, y_pred), False

def feval_pearsonr(y_pred, lgb_train):
    y_true = lgb_train.get_label()
    return 'pearsonr', stats.pearsonr(y_true, y_pred)[0], True

def calc_spread_return_per_day(df, portfolio_size=200, toprank_weight_ratio=2):
    assert df['Rank'].min() == 0
    assert df['Rank'].max() == len(df['Rank']) - 1
    weights = np.linspace(start=toprank_weight_ratio, stop=1, num=portfolio_size)
    purchase = (df.sort_values(by='Rank')['Target'][:portfolio_size] * weights).sum() / weights.mean()
    short = (df.sort_values(by='Rank', ascending=False)['Target'][:portfolio_size] * weights).sum() / weights.mean()
    return purchase - short

def calc_spread_return_sharpe(df: pd.DataFrame, portfolio_size=200, toprank_weight_ratio=2):
    buf = df.groupby('Date').apply(calc_spread_return_per_day, portfolio_size, toprank_weight_ratio)
    sharpe_ratio = buf.mean() / buf.std()
    return sharpe_ratio#, buf

def add_rank(df):
    df["Rank"] = df.groupby("Date")["Target"].rank(ascending=False, method="first") - 1 
    df["Rank"] = df["Rank"].astype("int")
    return df

def check_score(df,preds,Securities_filter=[]):
    tmp_preds=df[['Date','SecuritiesCode']].copy()
    tmp_preds['Target']=preds
    
    #Rank Filter. Calculate median for this date and assign this value to the list of Securities to filter.
    tmp_preds['target_mean']=tmp_preds.groupby("Date")["Target"].transform('median')
    tmp_preds.loc[tmp_preds['SecuritiesCode'].isin(Securities_filter),'Target']=tmp_preds['target_mean']
    
    tmp_preds = add_rank(tmp_preds)
    df['Rank']=tmp_preds['Rank']
    score=round(calc_spread_return_sharpe(df, portfolio_size= 200, toprank_weight_ratio= 2),5)
    score_mean=round(df.groupby('Date').apply(calc_spread_return_per_day, 200, 2).mean(),5)
    score_std=round(df.groupby('Date').apply(calc_spread_return_per_day, 200, 2).std(),5)
    print(f'Competition_Score:{score}, rank_score_mean:{score_mean}, rank_score_std:{score_std}')

#split dataset in features and target variable
X_column=list(DF1.filter(regex='^alpha').columns)
X = DF1[X_column] # alphas
y = DF1['Next-Return'] # tommorrow return
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)

"""### 普通方法判断重要因子"""

#Train the LGBM model using cross-validation
model = lgb.LGBMRegressor(random_state=4)
model.fit(X_train, y_train)
cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')
print(f'Cross-validated Mean Squared Error: {np.mean(np.abs(cv_scores))}')

#Calculate the feature importance and plot
feature_importances = model.feature_importances_
sorted_idx = feature_importances.argsort()

plt.figure(figsize=(10, 49))
plt.barh(X.columns[sorted_idx], feature_importances[sorted_idx])
plt.xlabel('Importance')
plt.title('Feature Importances')
plt.show()

feature_importances[sorted_idx]

feature_importances.sort()
feature_importances

X.columns[sorted_idx]

"""### lasso判断重要因子"""

from sklearn.preprocessing import StandardScaler
lasso = Lasso(alpha=0.01)

# Scale the input data X
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
lasso.fit(X_train_scaled, y_train)

# Get important features
important_features_lasso = X.columns[lasso.coef_ != 0]
X_train_filtered = X_train[important_features_lasso]
X_test_filtered = X_test[important_features_lasso]

print(np.isinf(X_train).any())

print(important_features_lasso)

"""### 二次训练

#### 常规特征筛选结果
"""

#Train the LGBM model again based on important features
# Select top n important features, you can adjust this value
n = 50
important_features = X.columns[sorted_idx][-n:]

X_train_important = X_train[important_features]
X_test_important = X_test[important_features]

model_important = lgb.LGBMRegressor(random_state=4)
model_important.fit(X_train_important, y_train)

cv_scores_important = cross_val_score(model_important, X_train_important, y_train, cv=5, scoring='neg_mean_squared_error')
print(f'Cross-validated Mean Squared Error (Important Features): {np.mean(np.abs(cv_scores_important))}')

important_features

for i in range(50,81,5):
  important_features = X.columns[sorted_idx][-n:]

  X_train_important = X_train[important_features]
  X_test_important = X_test[important_features]

  model_important = lgb.LGBMRegressor(random_state=4)
  cv_scores_important = cross_val_score(model_important, X_train_important, y_train, cv=5, scoring='neg_mean_squared_error')
  print('Cross-validated Mean Squared Error (Important Features): {} ,n is: {} '.format(np.mean(np.abs(cv_scores_important)), i))

  model_important.fit(X_train_important, y_train)

"""#### LASSO特征筛选结果"""

model_important_lasso = lgb.LGBMRegressor(random_state=4)
cv_scores_important = cross_val_score(model_important_lasso, X_train_filtered, y_train, cv=5, scoring='neg_mean_squared_error')
print(f'Cross-validated Mean Squared Error (Important Features): {np.mean(np.abs(cv_scores_important))}')

model_important_lasso.fit(X_train_important, y_train)

"""### 贝叶斯搜索超参数"""

kf = KFold(n_splits=5, shuffle=True, random_state=4)
def lgbm_cv_score(max_depth, learning_rate, n_estimators, num_leaves, min_child_samples, subsample, colsample_bytree, reg_alpha, reg_lambda):
    lgbm_params = {
        'max_depth': int(max_depth),
        'learning_rate': learning_rate,
        'n_estimators': int(n_estimators),
        'num_leaves': int(num_leaves),
        'min_child_samples': int(min_child_samples),
        'subsample': subsample,
        'colsample_bytree': colsample_bytree,
        'reg_alpha': reg_alpha,
        'reg_lambda': reg_lambda,
        'objective': 'regression',
        'n_jobs': -1,
        'random_state': 4,
    }

    lgbm_model = LGBMRegressor(**lgbm_params)
    cv_scores = []

    for train_index, val_index in kf.split(X_train_important):
        X_train_cv, X_val_cv = X_train.iloc[train_index], X_train.iloc[val_index]
        y_train_cv, y_val_cv = y_train.iloc[train_index], y_train.iloc[val_index]

        lgbm_model.fit(X_train_cv, y_train_cv, eval_set=[(X_val_cv, y_val_cv)], early_stopping_rounds=10, verbose=0)
        cv_scores.append(mean_squared_error(y_val_cv, lgbm_model.predict(X_val_cv))**0.5)

    return -np.mean(cv_scores)

from lightgbm import LGBMRegressor
bounds = {
    'max_depth': (5, 10),
    'learning_rate': (0.01, 0.1),
    'n_estimators': (50, 200),
    'num_leaves': (15, 50),
    'min_child_samples': (5, 30),
    'subsample': (0.6, 1),
    'colsample_bytree': (0.6, 1),
    'reg_alpha': (0, 1), 
    'reg_lambda': (0, 1),
}

optimizer = BayesianOptimization(f=lgbm_cv_score, pbounds=bounds, random_state=4)
optimizer.maximize(init_points=10, n_iter=15)

targets = [-r['target'] for r in optimizer.res]
plt.plot(targets)
plt.xlabel('Iteration')
plt.ylabel('CV Score')
plt.title('Bayesian Optimization Progress')
plt.show()

# Extract the values of 'max_depth' from the optimizer results
max_depths = [r['params']['max_depth'] for r in optimizer.res]

# Create a line plot showing how 'max_depth' changes over time
plt.plot(max_depths)
plt.xlabel('Iteration')
plt.ylabel('max_depth')
plt.title('Bayesian Optimization Progress')
plt.show()

# Define a list of hyperparameters to plot
hyperparams = ['max_depth', 'learning_rate', 'n_estimators', 'num_leaves']

# Extract the values of each hyperparameter from the optimizer results
values = {}
for param in hyperparams:
    values[param] = [r['params'][param] for r in optimizer.res]

# Create a line plot for each hyperparameter
fig, ax = plt.subplots()
for param in hyperparams:
    ax.plot(values[param], label=param)
ax.set_xlabel('Iteration')
ax.set_ylabel('Hyperparameter Value')
ax.set_title('Bayesian Optimization Progress')
ax.legend()
plt.show()

# Define a list of hyperparameters to plot
hyperparams = ['max_depth', 'learning_rate', 'num_leaves']

# Extract the values of each hyperparameter from the optimizer results
values = {}
for param in hyperparams:
    values[param] = [r['params'][param] for r in optimizer.res]

# Extract the values of 'n_estimators' from the optimizer results
n_estimators = [r['params']['n_estimators'] for r in optimizer.res]

# Create a line plot for each hyperparameter, with 'n_estimators' on a second y-axis
fig, ax1 = plt.subplots(figsize=(15,8))


# Plot the hyperparameters on the first y-axis (left)
for param in hyperparams:
    ax1.plot(values[param], label=param)
ax1.set_xlabel('Iteration')
ax1.set_ylabel('Hyperparameter Value')

# Create a second y-axis (right) for 'n_estimators'
ax2 = ax1.twinx()
ax2.plot(n_estimators, color='red', label='n_estimators')
ax2.set_ylabel('n_estimators')

# Display the legend for both y-axes
lines1, labels1 = ax1.get_legend_handles_labels()
lines2, labels2 = ax2.get_legend_handles_labels()
ax2.legend(lines1 + lines2, labels1 + labels2)

# Set the title for the plot
plt.title('Bayesian Optimization Progress')

# Show the plot
plt.show()

"""### 训练模型&early stop"""

optimized_params = optimizer.max['params']
optimized_params['max_depth'] = int(optimized_params['max_depth'])
optimized_params['n_estimators'] = int(optimized_params['n_estimators'])
optimized_params['num_leaves'] = int(optimized_params['num_leaves'])
optimized_params['min_child_samples'] = int(optimized_params['min_child_samples'])

final_lgbm_model = LGBMRegressor(**optimized_params)

best_early_stopping_round = None
best_cv_score = float('-inf')

for early_stopping_round in range(1, 51,5):
    # Perform cross-validation
    cv_scores = cross_val_score(final_lgbm_model, X_train_important, y_train, cv=kf, scoring='neg_mean_squared_error',
                                fit_params={'early_stopping_rounds': early_stopping_round,
                                            'eval_set': [(X_test_important, y_test)],
                                            'verbose': 0})

    mean_cv_score = np.mean(cv_scores)

    # Check if this early stopping round gives a better cross-validation score
    if mean_cv_score > best_cv_score:
        best_cv_score = mean_cv_score
        best_early_stopping_round = early_stopping_round

    print(f'Early stopping round: {early_stopping_round}, Cross-validation score: {mean_cv_score}')

print(f'Best early stopping round: {best_early_stopping_round}, Best cross-validation score: {best_cv_score}')

final_lgbm_model.fit(X_train_important, y_train, eval_set=[(X_test_important, y_test)], early_stopping_rounds=21, verbose=0)

"""### 模型评价"""

y_pred = final_lgbm_model.predict(X_test_important)
test_rmse = mean_squared_error(y_test, y_pred)**0.5
print(f"Test RMSE: {test_rmse}")

#Save the trained model:
booster = final_lgbm_model.booster_
booster.save_model('final_lgbm_model_original.txt')

#uploaded = files.upload()
#loaded_lgbm_model = lgb.Booster(model_file='lgbm_model.txt')